{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from contextlib import nullcontext\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import float_qparams_weight_only_qconfig, default_qconfig\n",
    "\n",
    "# Support absolute imports for a standalone script\n",
    "sys.path.insert(0, Path.cwd().parent.as_posix())\n",
    "\n",
    "from model import ModelArgs, Transformer, softmax_sum\n",
    "from tokenizer import Tokenizer\n",
    "from tinystories import get_tokenizer_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "start = \"\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "max_new_tokens = 25 # number of tokens generated in each sample\n",
    "temperature = 0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 300 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "tokenizer = \"\" # override the tokenizer model path\n",
    "seed = 1337\n",
    "\n",
    "# original model instance loaded from checkpoint (.pt file)\n",
    "orig_model_path = \"../out/softmax1-42m/ckpt.pt\"\n",
    "# quantized .bin model output filepath\n",
    "q_filepath = \"../out/quantized/softmax1-42m.pt\"\n",
    "\n",
    "tok_path = \"../out/softmax1-42m/\"\n",
    "\n",
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "def load_model(out_dir):\n",
    "    # init from a model saved in a specific directory\n",
    "    checkpoint_dict = torch.load(out_dir)\n",
    "    # del flash if exists\n",
    "    if 'flash' in checkpoint_dict['model_args']:\n",
    "        del checkpoint_dict['model_args']['flash']\n",
    "    # softmax -> softmax1 in model_args\n",
    "    if 'softmax' in checkpoint_dict['model_args']:\n",
    "        checkpoint_dict['model_args']['softmax1'] = checkpoint_dict['model_args']['softmax']\n",
    "        del checkpoint_dict['model_args']['softmax']\n",
    "    gptconf = ModelArgs(**checkpoint_dict['model_args'])\n",
    "    model = Transformer(gptconf)\n",
    "    state_dict = checkpoint_dict['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()    \n",
    "    return model, checkpoint_dict['model_args']\n",
    "\n",
    "def load_quant_model(dir):\n",
    "    # init from a model saved in a specific directory\n",
    "    checkpoint_dict = torch.load(dir)\n",
    "    # del flash if exists\n",
    "    if 'flash' in checkpoint_dict['model_args']:\n",
    "        del checkpoint_dict['model_args']['flash']\n",
    "    # softmax -> softmax1 in model_args\n",
    "    if 'softmax' in checkpoint_dict['model_args']:\n",
    "        checkpoint_dict['model_args']['softmax1'] = checkpoint_dict['model_args']['softmax']\n",
    "        del checkpoint_dict['model_args']['softmax']\n",
    "    gptconf = ModelArgs(**checkpoint_dict['model_args'])\n",
    "    model = Transformer(gptconf)\n",
    "    \n",
    "    # VERY IMPORTANT embeddings only support float_qparams_weight_only_qconfig quantization\n",
    "    model.tok_embeddings.qconfig = float_qparams_weight_only_qconfig\n",
    "    model_dynamic_quantized = torch.quantization.quantize_dynamic(\n",
    "        model, dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "    state_dict = checkpoint_dict['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model_dynamic_quantized.load_state_dict(state_dict, strict=True)\n",
    "    model_dynamic_quantized.eval()\n",
    "    return model_dynamic_quantized, checkpoint_dict['model_args']\n",
    "\n",
    "def load_tokenizer(out_dir):\n",
    "    # load the tokenizer\n",
    "    with open(f\"{out_dir}/config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "    vocab_source = config.get(\"vocab_source\", \"llama2\")\n",
    "    vocab_size = config['vocab_size']\n",
    "    if tokenizer:\n",
    "        # a specific tokenizer is provided, use it\n",
    "        tokenizer_model = tokenizer\n",
    "    else:\n",
    "        # let's try to find the tokenizer model automatically. bit gross here...\n",
    "        query_vocab_size = 0 if vocab_source == \"llama2\" else vocab_size\n",
    "        tokenizer_model = get_tokenizer_model_path(vocab_size=query_vocab_size)\n",
    "    enc = Tokenizer(tokenizer_model='../tokenizer.model')\n",
    "    return enc\n",
    "\n",
    "enc = load_tokenizer(tok_path)\n",
    "\n",
    "def encode_prompt(start, device):\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "    start_ids = enc.encode(start, bos=True, eos=False)\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "    return x\n",
    "\n",
    "# run generation\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt, device, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k):\n",
    "    x = encode_prompt(prompt, device=device)\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_new_tokens, temperature, top_k)\n",
    "        return enc.decode(y.tolist())[0]\n",
    "\n",
    "def compute_perplexity(prompt, model, device, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k):\n",
    "    \"\"\"\n",
    "    Compute the perplexity given the logits of generated tokens and their corresponding indices.\n",
    "    \"\"\"\n",
    "    x = encode_prompt(prompt, device=device)\n",
    "    idx, logits = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, return_logits=True)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    actual_probs = torch.gather(probs, -1, idx.unsqueeze(-1)[:, x.shape[1]:, :])\n",
    "    neg_log_probs = -torch.log(actual_probs)\n",
    "    return torch.exp(torch.mean(neg_log_probs)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of existing model to 8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "168.88 MB\n",
      "60.52 MB\n"
     ]
    }
   ],
   "source": [
    "model, config = load_model(orig_model_path)\n",
    "# VERY IMPORTANT embeddings only support float_qparams_weight_only_qconfig quantization\n",
    "model.tok_embeddings.qconfig = float_qparams_weight_only_qconfig\n",
    "model_dynamic_quantized = torch.quantization.quantize_dynamic(\n",
    "    model, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print_model_size(model)\n",
    "print_model_size(model_dynamic_quantized)\n",
    "\n",
    "# save the quantized model\n",
    "checkpoint_dict = {\n",
    "    'model_args': config,\n",
    "    'model': model_dynamic_quantized.state_dict()\n",
    "}\n",
    "torch.save(checkpoint_dict, q_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPL comparison between unquantized and quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pyn/anaconda3/envs/softmaxn/lib/python3.9/site-packages/torch/_utils.py:376: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "WARNING: using slow attention for softmax-n. Appeals for flashattention will be IGNORED!!!\n",
      "168.88 MB\n",
      "60.52 MB\n"
     ]
    }
   ],
   "source": [
    "orig_model, config = load_model(orig_model_path)\n",
    "quant_model, config = load_quant_model(q_filepath)\n",
    "\n",
    "# only unquantized models can run on GPU\n",
    "orig_model = orig_model.to('cuda')\n",
    "quant_model = quant_model.to('cpu')\n",
    "\n",
    "print_model_size(orig_model)\n",
    "print_model_size(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5473077297210693\n",
      "2.383082628250122\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Sally sold seashells\"\n",
    "max_new_tokens = 20\n",
    "\n",
    "orig_ppl = compute_perplexity(prompt, orig_model, device='cuda', max_new_tokens=max_new_tokens)\n",
    "print(orig_ppl)\n",
    "\n",
    "quant_ppl = compute_perplexity(prompt, quant_model, device=next(quant_model.parameters()).device, max_new_tokens=max_new_tokens)\n",
    "print(quant_ppl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softermax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
