{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import float_qparams_weight_only_qconfig, default_qconfig\n",
    "\n",
    "# Support absolute imports for a standalone script\n",
    "sys.path.insert(0, Path.cwd().parent.as_posix())\n",
    "\n",
    "from tinystories import get_tokenizer_model_path  # noqa: E402\n",
    "from tokenizer import Tokenizer  # noqa: E402\n",
    "from model import ModelArgs, Transformer  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import functools\n",
    "import py._io.capture\n",
    "import py._io\n",
    "import py\n",
    "\n",
    "start = (\n",
    "    \"\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    ")\n",
    "max_new_tokens = 25  # number of tokens generated in each sample\n",
    "temperature = (\n",
    "    0  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    ")\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "top_k = (\n",
    "    300  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    ")\n",
    "tokenizer = \"\"  # override the tokenizer model path\n",
    "seed = 1337\n",
    "\n",
    "# original model instance loaded from checkpoint (.pt file)\n",
    "orig_model_path = \"../out/softmax0-15m-2023_08_26_00_08_49/ckpt.pt\"\n",
    "# quantized .bin model output filepath\n",
    "q_filepath = \"../out/quantized/softmax0-15m-2023_08_26_00_08_49.pt\"\n",
    "# Path to tokenizer (config.json)\n",
    "tok_path = \"../out/softmax0-15m-2023_08_26_00_08_49/\"\n",
    "\n",
    "\n",
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" % (os.path.getsize(\"tmp.pt\") / 1e6))\n",
    "    os.remove(\"tmp.pt\")\n",
    "\n",
    "\n",
    "def load_model(out_dir):\n",
    "    # init from a model saved in a specific directory\n",
    "    checkpoint_dict = torch.load(out_dir, map_location=device)\n",
    "    # del flash if exists\n",
    "    if \"flash\" in checkpoint_dict[\"model_args\"]:\n",
    "        del checkpoint_dict[\"model_args\"][\"flash\"]\n",
    "    # softmax -> softmax1 in model_args\n",
    "    if \"softmax\" in checkpoint_dict[\"model_args\"]:\n",
    "        checkpoint_dict[\"model_args\"][\"softmax1\"] = checkpoint_dict[\"model_args\"][\n",
    "            \"softmax\"\n",
    "        ]\n",
    "        del checkpoint_dict[\"model_args\"][\"softmax\"]\n",
    "    gptconf = ModelArgs(**checkpoint_dict[\"model_args\"])\n",
    "    model = Transformer(gptconf)\n",
    "    state_dict = checkpoint_dict[\"model\"]\n",
    "    unwanted_prefix = \"_orig_mod.\"\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model, checkpoint_dict[\"model_args\"]\n",
    "\n",
    "\n",
    "def load_quant_model(dir):\n",
    "    # init from a model saved in a specific directory\n",
    "    checkpoint_dict = torch.load(dir)\n",
    "    # del flash if exists\n",
    "    if \"flash\" in checkpoint_dict[\"model_args\"]:\n",
    "        del checkpoint_dict[\"model_args\"][\"flash\"]\n",
    "    # softmax -> softmax1 in model_args\n",
    "    if \"softmax\" in checkpoint_dict[\"model_args\"]:\n",
    "        checkpoint_dict[\"model_args\"][\"softmax1\"] = checkpoint_dict[\"model_args\"][\n",
    "            \"softmax\"\n",
    "        ]\n",
    "        del checkpoint_dict[\"model_args\"][\"softmax\"]\n",
    "    gptconf = ModelArgs(**checkpoint_dict[\"model_args\"])\n",
    "    model = Transformer(gptconf)\n",
    "\n",
    "    # VERY IMPORTANT embeddings only support float_qparams_weight_only_qconfig quantization\n",
    "    model.tok_embeddings.qconfig = float_qparams_weight_only_qconfig\n",
    "    model_dynamic_quantized = torch.quantization.quantize_dynamic(\n",
    "        model, dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "    state_dict = checkpoint_dict[\"model\"]\n",
    "    unwanted_prefix = \"_orig_mod.\"\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model_dynamic_quantized.load_state_dict(state_dict, strict=True)\n",
    "    model_dynamic_quantized.eval()\n",
    "    return model_dynamic_quantized, checkpoint_dict[\"model_args\"]\n",
    "\n",
    "\n",
    "def load_tokenizer(out_dir):\n",
    "    # load the tokenizer\n",
    "    with open(f\"{out_dir}/config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    vocab_source = config.get(\"vocab_source\", \"llama2\")\n",
    "    vocab_size = config[\"vocab_size\"]\n",
    "    if tokenizer:\n",
    "        # a specific tokenizer is provided, use it\n",
    "        tokenizer_model = tokenizer\n",
    "    else:\n",
    "        # let's try to find the tokenizer model automatically. bit gross here...\n",
    "        query_vocab_size = 0 if vocab_source == \"llama2\" else vocab_size\n",
    "        tokenizer_model = get_tokenizer_model_path(vocab_size=query_vocab_size)\n",
    "    enc = Tokenizer(tokenizer_model=\"../tokenizer.model\")\n",
    "    return enc\n",
    "\n",
    "\n",
    "enc = load_tokenizer(tok_path)\n",
    "\n",
    "\n",
    "def encode_prompt(start, device):\n",
    "    if start.startswith(\"FILE:\"):\n",
    "        with open(start[5:], \"r\", encoding=\"utf-8\") as f:\n",
    "            start = f.read()\n",
    "    start_ids = enc.encode(start, bos=True, eos=False)\n",
    "    x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
    "    return x\n",
    "\n",
    "\n",
    "# run generation\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    prompt,\n",
    "    device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "):\n",
    "    x = encode_prompt(prompt, device=device)\n",
    "    with ctx:\n",
    "        y = model.generate(x, max_new_tokens, temperature, top_k)\n",
    "        return enc.decode(y.tolist())[0]\n",
    "\n",
    "\n",
    "def get_capture(out, in_):\n",
    "    try:\n",
    "        capture = py.io.StdCaptureFD(out=out, in_=in_)\n",
    "    except:\n",
    "        capture = None\n",
    "    return capture\n",
    "\n",
    "\n",
    "def reset_capture(capture):\n",
    "    if capture is not None:\n",
    "        capture.reset()\n",
    "\n",
    "\n",
    "def hide_warnings(function=None, out=True, in_=False):\n",
    "    \"\"\"Suppresses C++ warnings in PyTorch underlying methods. Decorate on functions\"\"\"\n",
    "\n",
    "    def decorator_hide_warnings(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            capture = get_capture(out, in_)\n",
    "            result = func(*args, **kwargs)\n",
    "            reset_capture(capture)\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    if function:\n",
    "        return decorator_hide_warnings(function)\n",
    "    return decorator_hide_warnings\n",
    "\n",
    "\n",
    "@hide_warnings\n",
    "def compute_perplexity(\n",
    "    prompt,\n",
    "    model,\n",
    "    device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the perplexity given the logits of generated tokens and their corresponding indices.\n",
    "    \"\"\"\n",
    "    x = encode_prompt(prompt, device=device)\n",
    "    idx, logits = model.generate(\n",
    "        x,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        return_logits=True,\n",
    "    )\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    actual_probs = torch.gather(\n",
    "        probs, -1, idx.unsqueeze(-1)[:, x.shape[1]:, :])\n",
    "    neg_log_probs = -torch.log(actual_probs)\n",
    "\n",
    "    y = enc.decode(idx.tolist())[0]\n",
    "    return y, torch.exp(torch.mean(neg_log_probs)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating parameter count of layers\n",
    "\n",
    "Which weight matrices are the biggest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings param count\n",
      "36.86 MB\n",
      "Transformer param count\n",
      "25.49 MB\n",
      "Total param count\n",
      "62.36 MB\n"
     ]
    }
   ],
   "source": [
    "model, config = load_model(orig_model_path)\n",
    "\n",
    "# Embeddings param count\n",
    "print(\"Embeddings param count\")\n",
    "print_model_size(model.tok_embeddings)  # only 1 matrix cause weight tying\n",
    "\n",
    "# Transformer param count\n",
    "print(\"Transformer param count\")\n",
    "print_model_size(model.layers)\n",
    "\n",
    "# Total param count\n",
    "print(\"Total param count\")\n",
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of existing model to 8bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.36 MB\n",
      "26.51 MB\n",
      "9.47 MB\n",
      "7.69 MB\n"
     ]
    }
   ],
   "source": [
    "model, config = load_model(orig_model_path)\n",
    "# VERY IMPORTANT embeddings only support float_qparams_weight_only_qconfig quantization\n",
    "model.tok_embeddings.qconfig = float_qparams_weight_only_qconfig\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "model_dynamic_quantized = torch.quantization.quantize_dynamic(\n",
    "    model, dtype=torch.qint8)\n",
    "\n",
    "print_model_size(model)\n",
    "print_model_size(model_dynamic_quantized)\n",
    "\n",
    "print_model_size(model_dynamic_quantized.tok_embeddings)\n",
    "print_model_size(model_dynamic_quantized.layers)\n",
    "\n",
    "# save the quantized model\n",
    "checkpoint_dict = {\"model_args\": config,\n",
    "                   \"model\": model_dynamic_quantized.state_dict()}\n",
    "torch.save(checkpoint_dict, q_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPL comparison between unquantized and quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.36 MB\n",
      "26.51 MB\n"
     ]
    }
   ],
   "source": [
    "orig_model, config = load_model(orig_model_path)\n",
    "quant_model, config = load_quant_model(q_filepath)\n",
    "\n",
    "# only unquantized models can run on GPU\n",
    "orig_model = orig_model.to(\"cpu\")\n",
    "quant_model = quant_model.to(\"cpu\")\n",
    "\n",
    "print_model_size(orig_model)\n",
    "print_model_size(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sally sold seashells. She had a big bag of seashells and she wanted to sell them. She put the seashells in a', 1.8265522718429565)\n",
      "('Sally sold seashells. She was very happy. She had a big pile of seashells in her garden. She was so proud of', 2.160936117172241)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "prompt = \"Sally sold seashells\"\n",
    "max_new_tokens = 20\n",
    "\n",
    "orig_ppl = compute_perplexity(prompt, orig_model, device=\"cpu\")\n",
    "\n",
    "quant_ppl = compute_perplexity(\n",
    "    prompt,\n",
    "    quant_model,\n",
    "    device=next(quant_model.parameters()).device,\n",
    ")\n",
    "\n",
    "print(orig_ppl)\n",
    "print(quant_ppl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softermax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
